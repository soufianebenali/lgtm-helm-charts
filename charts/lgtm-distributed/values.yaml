---
grafana:
  # -- Deploy Grafana if enabled. See [upstream readme](https://github.com/grafana/helm-charts/tree/main/charts/grafana#configuration) for full values reference.
  enabled: true

  # -- Grafana data sources config. Connects to all three by default
  datasources:
    datasources.yaml:
      apiVersion: 1
      # -- Datasources linked to the Grafana instance. Override if you disable any components.
      datasources:
        # https://grafana.com/docs/grafana/latest/datasources/loki/#provision-the-loki-data-source
        - name: Loki
          uid: loki
          type: loki
          url: http://{{ .Release.Name }}-loki-gateway
          isDefault: false
        # https://grafana.com/docs/grafana/latest/datasources/prometheus/#provision-the-data-source
        - name: Mimir
          uid: prom
          type: prometheus
          url: http://{{ .Release.Name }}-mimir-nginx/prometheus
          isDefault: true
        # https://grafana.com/docs/grafana/latest/datasources/tempo/configure-tempo-data-source/#provision-the-data-source
        - name: Tempo
          uid: tempo
          type: tempo
          url: http://{{ .Release.Name }}-tempo-query-frontend:3100
          isDefault: false
          jsonData:
            tracesToLogsV2:
              datasourceUid: loki
            lokiSearch:
              datasourceUid: loki
            tracesToMetrics:
              datasourceUid: prom
            serviceMap:
              datasourceUid: prom

loki:
  # -- Deploy Loki if enabled. See [upstream readme](https://github.com/grafana/helm-charts/tree/main/charts/loki-distributed#values) for full values reference.
  enabled: true
  loki:
    structuredConfig:
      memberlist:
        bind_addr: []
  ingester:
    extraArgs: 
      - -config.expand-env=true
      - -memberlist.bind-addr=$(MY_POD_IP_LOKI)
    extraEnv:
    - name: MY_POD_IP_LOKI
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  distributor:
    extraArgs: 
    - -memberlist.bind-addr=$(MY_POD_IP_LOKI)
    extraEnv:
    - name: MY_POD_IP_LOKI
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  querier:
    extraArgs: 
    - -memberlist.bind-addr=$(MY_POD_IP_LOKI)
    # -- Environment variables to add to the ingester pods
    extraEnv:
    - name: MY_POD_IP_LOKI
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  serviceMonitor:
    enabled: true
# -- Mimir chart values. Resources are set to a minimum by default.
mimir:
  # -- Deploy Mimir if enabled. See [upstream values.yaml](https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/values.yaml) for full values reference.
  enabled: true
  mimir:
      config: |
        usage_stats:
          installation_mode: helm

        activity_tracker:
          filepath: /active-query-tracker/activity.log

        {{- if .Values.enterprise.enabled }}
        admin_api:
          leader_election:
            enabled: true
            ring:
              kvstore:
                store: "memberlist"

        admin_client:
          storage:
          {{- if .Values.minio.enabled }}
            type: s3
            s3:
              access_key_id: {{ .Values.minio.rootUser }}
              bucket_name: enterprise-metrics-admin
              endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
              insecure: true
              secret_access_key: {{ .Values.minio.rootPassword }}
          {{- end }}
          {{- if (index .Values "admin-cache" "enabled") }}
            cache:
              backend: memcached
              memcached:
                addresses: {{ include "mimir.adminCacheAddress" . }}
                max_item_size: {{ mul (index .Values "admin-cache").maxItemMemory 1024 1024 }}
          {{- end }}
        {{- end }}

        alertmanager:
          data_dir: /data
          enable_api: true
          external_url: /alertmanager
          {{- if .Values.alertmanager.zoneAwareReplication.enabled }}
          sharding_ring:
            zone_awareness_enabled: true
          {{- end }}
          {{- if .Values.alertmanager.fallbackConfig }}
          fallback_config_file: /configs/alertmanager_fallback_config.yaml
          {{- end }}

        {{- if .Values.minio.enabled }}
        alertmanager_storage:
          backend: s3
          s3:
            access_key_id: {{ .Values.minio.rootUser }}
            bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
            endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
            insecure: true
            secret_access_key: {{ .Values.minio.rootPassword }}
        {{- end }}

        {{- if .Values.enterprise.enabled }}
        auth:
          type: enterprise
        {{- end }}

        # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.
        blocks_storage:
          backend: s3
          bucket_store:
            {{- if index .Values "chunks-cache" "enabled" }}
            chunks_cache:
              backend: memcached
              memcached:
                addresses: {{ include "mimir.chunksCacheAddress" . }}
                max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
                timeout: 450ms
                max_idle_connections: 150
            {{- end }}
            {{- if index .Values "index-cache" "enabled" }}
            index_cache:
              backend: memcached
              memcached:
                addresses: {{ include "mimir.indexCacheAddress" . }}
                max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
                timeout: 450ms
                max_idle_connections: 150
            {{- end }}
            {{- if index .Values "metadata-cache" "enabled" }}
            metadata_cache:
              backend: memcached
              memcached:
                addresses: {{ include "mimir.metadataCacheAddress" . }}
                max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
                max_idle_connections: 150
            {{- end }}
            sync_dir: /data/tsdb-sync
          {{- if .Values.minio.enabled }}
          s3:
            access_key_id: {{ .Values.minio.rootUser }}
            bucket_name: {{ include "mimir.minioBucketPrefix" . }}-tsdb
            endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
            insecure: true
            secret_access_key: {{ .Values.minio.rootPassword }}
          {{- end }}
          tsdb:
            dir: /data/tsdb
            head_compaction_interval: 15m
            wal_replay_concurrency: 3

        {{- if .Values.enterprise.enabled }}
        cluster_name: "{{ .Release.Name }}"
        {{- end }}

        compactor:
          compaction_interval: 30m
          deletion_delay: 2h
          max_closing_blocks_concurrency: 2
          max_opening_blocks_concurrency: 4
          symbols_flushers_concurrency: 4
          first_level_compaction_wait_period: 25m
          data_dir: "/data"
          sharding_ring:
            wait_stability_min_duration: 1m
            heartbeat_period: 1m
            heartbeat_timeout: 4m

        distributor:
          ring:
            heartbeat_period: 1m
            heartbeat_timeout: 4m

        frontend:
          parallelize_shardable_queries: true
          {{- if index .Values "results-cache" "enabled" }}
          results_cache:
            backend: memcached
            memcached:
              timeout: 500ms
              addresses: {{ include "mimir.resultsCacheAddress" . }}
              max_item_size: {{ mul (index .Values "results-cache").maxItemMemory 1024 1024 }}
          cache_results: true
          query_sharding_target_series_per_shard: 2500
          {{- end }}
          {{- if .Values.query_scheduler.enabled }}
          scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
          {{- end }}

        frontend_worker:
          grpc_client_config:
            max_send_msg_size: 419430400 # 400MiB
          {{- if .Values.query_scheduler.enabled }}
          scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
          {{- else }}
          frontend_address: {{ template "mimir.fullname" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
          {{- end }}

        {{- if and .Values.enterprise.enabled }}
        gateway:
          proxy:
            admin_api:
              url: http://{{ template "mimir.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            alertmanager:
              url: http://{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            compactor:
              url: http://{{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            default:
              url: http://{{ template "mimir.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            distributor:
              url: dns:///{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverGrpcListenPort" . }}
            ingester:
              url: http://{{ template "mimir.fullname" . }}-ingester-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            query_frontend:
              url: http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            ruler:
              url: http://{{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            store_gateway:
              url: http://{{ template "mimir.fullname" . }}-store-gateway-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}
            graphite_write_proxy:
              url: http://{{ template "mimir.fullname" . }}-graphite-write-proxy.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            graphite_querier:
              url: http://{{ template "mimir.fullname" . }}-graphite-querier.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" . }}
            {{- end}}
        {{- end }}

        ingester:
          ring:
            final_sleep: 0s
            num_tokens: 512
            tokens_file_path: /data/tokens
            unregister_on_shutdown: false
            heartbeat_period: 2m
            heartbeat_timeout: 10m
            {{- if .Values.ingester.zoneAwareReplication.enabled }}
            zone_awareness_enabled: true
            {{- end }}

        ingester_client:
          grpc_client_config:
            max_recv_msg_size: 104857600
            max_send_msg_size: 104857600

        {{- if .Values.enterprise.enabled }}
        instrumentation:
          enabled: true
          distributor_client:
            address: dns:///{{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.serverGrpcListenPort" . }}

        license:
          path: "/license/license.jwt"
        {{- end }}

        limits:
          # Limit queries to 500 days. You can override this on a per-tenant basis.
          max_total_query_length: 12000h
          # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
          # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
          max_query_parallelism: 240
          # Avoid caching results newer than 10m because some samples can be delayed
          # This presents caching incomplete results
          max_cache_freshness: 10m

        memberlist:
          abort_if_cluster_join_fails: false
          compression_enabled: false
          bind_addr:
          - ${MY_POD_IP}
          join_members:
          - {{ include "mimir.fullname" . }}-gossip-ring:7946

        querier:
          # With query sharding we run more but smaller queries. We must strike a balance
          # which allows us to process more sharded queries in parallel when requested, but not overload
          # queriers during non-sharded queries.
          max_concurrent: 16

        query_scheduler:
          # Increase from default of 100 to account for queries created by query sharding
          max_outstanding_requests_per_tenant: 800

        ruler:
          alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
          enable_api: true
          rule_path: /data

        {{- if or (.Values.minio.enabled) (index .Values "metadata-cache" "enabled") }}
        ruler_storage:
          {{- if .Values.minio.enabled }}
          backend: s3
          s3:
            endpoint: {{ .Release.Name }}-minio.{{ .Release.Namespace }}.svc:9000
            bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
            access_key_id: {{ .Values.minio.rootUser }}
            secret_access_key: {{ .Values.minio.rootPassword }}
            insecure: true
          {{- end }}
          {{- if index .Values "metadata-cache" "enabled" }}
          cache:
            backend: memcached
            memcached:
              addresses: {{ include "mimir.metadataCacheAddress" . }}
              max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
          {{- end }}
        {{- end }}

        runtime_config:
          file: /var/{{ include "mimir.name" . }}/runtime.yaml

        store_gateway:
          sharding_ring:
            heartbeat_period: 1m
            heartbeat_timeout: 4m
            wait_stability_min_duration: 1m
            {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
            kvstore:
              prefix: multi-zone/
            {{- end }}
            tokens_file_path: /data/tokens
            unregister_on_shutdown: false
            {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
            zone_awareness_enabled: true
            {{- end }}

        {{- if and .Values.enterprise.enabled .Values.graphite.enabled }}
        graphite:
          enabled: true

          write_proxy:
            distributor_client:
              address: dns:///{{ template "mimir.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" .  }}

          querier:
            remote_read:
              query_address: http://{{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "mimir.serverHttpListenPort" .  }}/prometheus
            proxy_bad_requests: false

            schemas:
              default_storage_schemas_file: /etc/graphite-proxy/storage-schemas.conf
              default_storage_aggregations_file: /etc/graphite-proxy/storage-aggregations.conf
            aggregation_cache:
              memcached:
                addresses: dnssrvnoa+{{ template "mimir.fullname" . }}-gr-aggr-cache.{{ .Release.Namespace}}.svc:11211
                timeout: 1s
            metric_name_cache:
              memcached:
                addresses: dnssrvnoa+{{ template "mimir.fullname" . }}-gr-metricname-cache.{{ .Release.Namespace}}.svc:11211
                timeout: 1s
        {{- end}}

  global:
    extraEnv:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  alertmanager:
    resources:
      requests:
        cpu: 20m
  compactor:
    resources:
      requests:
        cpu: 20m
  distributor:
    resources:
      requests:
        cpu: 20m
  ingester:
    replicas: 2
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 20m
  overrides_exporter:
    resources:
      requests:
        cpu: 20m
  querier:
    replicas: 1
    resources:
      requests:
        cpu: 20m
  query_frontend:
    resources:
      requests:
        cpu: 20m
  query_scheduler:
    replicas: 1
    resources:
      requests:
        cpu: 20m
  ruler:
    resources:
      requests:
        cpu: 20m
  store_gateway:
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 20m
  minio:
    resources:
      requests:
        cpu: 20m
  rollout_operator:
    resources:
      requests:
        cpu: 20m
  nginx:
    nginxConfig:
          file: |
            worker_processes  5;  ## Default: 1
            error_log  /dev/stderr {{ .Values.nginx.nginxConfig.errorLogLevel }};
            pid        /tmp/nginx.pid;
            worker_rlimit_nofile 8192;

            events {
              worker_connections  4096;  ## Default: 1024
            }

            http {
              client_body_temp_path /tmp/client_temp;
              proxy_temp_path       /tmp/proxy_temp_path;
              fastcgi_temp_path     /tmp/fastcgi_temp;
              uwsgi_temp_path       /tmp/uwsgi_temp;
              scgi_temp_path        /tmp/scgi_temp;

              default_type application/octet-stream;
              log_format   {{ .Values.nginx.nginxConfig.logFormat }}

              {{- if .Values.nginx.verboseLogging }}
              access_log   /dev/stderr  main;
              {{- else }}

              map $status $loggable {
                ~^[23]  0;
                default 1;
              }
              access_log   {{ .Values.nginx.nginxConfig.accessLogEnabled | ternary "/dev/stderr  main  if=$loggable;" "off;" }}
              {{- end }}

              sendfile           on;
              tcp_nopush         on;
              proxy_http_version 1.1;

              {{- if .Values.nginx.nginxConfig.resolver }}
              resolver {{ .Values.nginx.nginxConfig.resolver }};
              {{- else }}
              resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
              {{- end }}

              {{- with .Values.nginx.nginxConfig.httpSnippet }}
              {{ . | nindent 2 }}
              {{- end }}

              # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
              map $http_x_scope_orgid $ensured_x_scope_orgid {
                default $http_x_scope_orgid;
                "" "{{ include "mimir.noAuthTenant" . }}";
              }

              proxy_read_timeout 300;
              server {
                listen 8080;

                {{- if .Values.nginx.basicAuth.enabled }}
                auth_basic           "Mimir";
                auth_basic_user_file /etc/nginx/secrets/.htpasswd;
                {{- end }}

                location = / {
                  return 200 'OK';
                  auth_basic off;
                }

                proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

                # Distributor endpoints
                location /distributor {
                  set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }
                location = /api/v1/push {
                  set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }
                location /otlp/v1/metrics {
                  set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                # Alertmanager endpoints
                location {{ template "mimir.alertmanagerHttpPrefix" . }} {
                  set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }
                location = /multitenant_alertmanager/status {
                  set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }
                location = /api/v1/alerts {
                  set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                # Ruler endpoints
                location {{ template "mimir.prometheusHttpPrefix" . }}/config/v1/rules {
                  set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }
                location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/rules {
                  set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/alerts {
                  set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }
                location = /ruler/ring {
                  set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                # Rest of {{ template "mimir.prometheusHttpPrefix" . }} goes to the query frontend
                location {{ template "mimir.prometheusHttpPrefix" . }} {
                  set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                # Buildinfo endpoint can go to any component
                location = /api/v1/status/buildinfo {
                  set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                # Compactor endpoint for uploading blocks
                location /api/v1/upload/block/ {
                  set $compactor {{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
                  proxy_pass      http://$compactor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
                }

                {{- with .Values.nginx.nginxConfig.serverSnippet }}
                {{ . | nindent 4 }}
                {{- end }}
              }
            }
  metaMonitoring:
    grafanaAgent:
      enabled: true
    dashboards:
    # -- If enabled, Grafana dashboards are deployed
      enabled: true
    serviceMonitor:
      enabled: true
    prometheusRule:
      enabled: true
      mimirRules: true
  graphite:
    enabled: true

tempo:
  # -- Deploy Tempo if enabled.  See [upstream readme](https://github.com/grafana/helm-charts/blob/main/charts/tempo-distributed/README.md#values) for full values reference.
  enabled: true
  ingester:
    extraArgs:
      - -config.expand-env=true
    # -- Environment variables to add to the ingester pods
    extraEnv:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP  
    replicas: 1
  memberlist:
    bind_addr:
    - ${MY_POD_IP}
  metaMonitoring:
    serviceMonitor:
      enabled: true

